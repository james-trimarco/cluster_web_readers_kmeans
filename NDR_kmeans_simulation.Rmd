---
title: "Clustering Web Readers With Kmeans"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})

author: "James Trimarco"
date: "2/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction  
Back when I was a magazine editor, I spent a lot of time looking over web traffic with my teammates. We could see patterns in the traffic and we often imagined that readers might fall into different categories. For example, some people mostly read lifestyle stories but also read about the environment; others who read all the politics stories but avoided arts and culture stories; and so on. If we could sort readers into these categories, we might be able to better personalize our communications with them. 

We didn't quite have a way to quantify these hunches at the time. 

After going back to school for data science, I realized that the kmeans algorithm offers a tool to do the classification we'd wanted. Kmeans starts by randomly selecting $k$ center points — $k$ is a number that we pick at the beginning of the process — and then runs an routine that finds all the points closest to each center, takes the average of those points to find a new center, and repeats until stable clusters emerge. It's an "unsupervised" algorithm, which means we won't know what categories we're going to find until we actually find them (For a full description of how kmeans works, I recommend this [fun video by StatQuest](https://www.youtube.com/watch?v=4b5d3muPQmA)). 

Since that time, I've run the kmeans algorithm for clients to help them identify clusters of subscribers hidden in their traffic data. Usually some results match the publishers' hunches — "We always thought we had a group that came around every time our star blogger posted new content but didn't read anything else." But other results came as a surprise. 

The following workflow shows the basics of how to apply the kmeans algorithm to web traffic data using some made-up data that roughly simulates typical client weblogs. 

## Part I: Setup
### Load libraries
```{r libraries}
library(tidyverse) # essential data tools
library(cluster) # clustering algorithms
library(fpc) # for visualizing clusters
library(wesanderson) # for colors
```


### Control panel
Various important settings are determined here. 

The four groups of clients we create in the code chunk below represent clusters with different reading habits. In an effort to simulate real web traffic data, the clusters are not all the same size. Furthermore, some clusters read more than others, and that number is independent of the number of readers in the cluster. 
```{r}
# Here's where we set the "k" in kmeans. 
# At the end, we'll see a number of clusters equal to the number we set k to here. 
k <- 3

# This sets the number of unique readers to generate.
# Eacher reader will get a subscriber code.
total_subscribers <- 1000

# This sets the number of weblogs the algorithm will examine.
# On average, each subscriber read 50 pieces of content during this time period.
total_weblogs <- 50000

# These settings determine how many of the clients fall into each cluster.
group_1_cutoff <- (0:250)
group_2_cutoff <- (251:600)
group_3_cutoff <- (601:750)
group_4_cutoff <- c(751:1000)

# These settings determine the number of weblogs made by each 
# group of readers. 
# The coefficients here should sum to 1.
group_1_logs <- .4*total_weblogs
group_2_logs <- .3*total_weblogs
group_3_logs <- .2*total_weblogs
group_4_logs <- .1*total_weblogs
```

## Part II: Synthesize weblog data
### Create publication categories
Imagine a blog that publishes recipes, and has different channels for different types of food. The probability vectors below define the proportions with which members of each group read the articles in different channels. 

Group Four doesn't have a probability matrix; its readers select from all the categories equally. 

Note that some stories are coded NA, which is something you see a lot in real client data. 

```{r}
channels <- c("soups", 
             "sandwiches", 
             "salads", 
             "barbecue", 
             "seafood", 
             "stir_fry", 
             NA)

# Create probability vectors
# Each of these sums to one
pr_group_1 <- c(.4, .3, .1, .05, .05, .025, .075)
pr_group_2 <- c(.13, .025, .05, .01, .55, .06, .175)
pr_group_3 <- c(.01, .01, .6, .01, .04, .01, .07)
```


### Create alphanumeric subscriber codes
The subscriber codes have six positions, with three letters and three numbers. 
```{r}
alpha_bit <- replicate(total_subscribers, paste(sample(LETTERS, 3, replace=TRUE), collapse=""))
numeric_bit <- replicate(total_subscribers, paste(sample(c(1:9), 3, replace=TRUE), collapse=""))
subscriber_codes <- paste0(alpha_bit, numeric_bit)
```

### Create column of dates
```{r}
# This function will generate a uniform sample of dates from 
# within a designated start and end date:
rand.date=function(start.day,end.day,size){   
  days=seq.Date(as.Date(start.day),as.Date(end.day),by="day")  
  pick.day=runif(n=size,min=1,max=length(days)) 
  date=days[pick.day]  
}
```

### Put it together into a data frame
This step glues the vectors we've been building into a data frame where each observation is a website visit. Each row has a subscriber code, a date, and a channel showing the kind of recipe the subscriber was reading.  
```{r}
group_1 <- tibble(sub_code = sample(subscriber_codes[group_1_cutoff], size=group_1_logs, replace = TRUE), 
               sys_time = rand.date("2014-01-01","2018-12-31",group_1_logs), 
               channel = sample(channels, size=group_1_logs, prob = pr_group_1, replace = TRUE))

group_2 <- tibble(sub_code = sample(subscriber_codes[group_2_cutoff], size=group_2_logs, replace = TRUE), 
               sys_time = rand.date("2014-01-01","2018-12-31",group_2_logs), 
               channel = sample(channels, size=group_2_logs, prob = pr_group_2, replace = TRUE))

group_3 <- tibble(sub_code = sample(subscriber_codes[group_3_cutoff], size=group_3_logs, replace = TRUE), 
               sys_time = rand.date("2014-01-01","2018-12-31",group_3_logs), 
               channel = sample(channels, size=group_3_logs, prob = pr_group_3, replace = TRUE))

group_4 <- tibble(sub_code = sample(subscriber_codes[group_4_cutoff], size=group_4_logs, replace = TRUE), 
               sys_time = rand.date("2014-01-01","2018-12-31",group_4_logs), 
               channel = sample(channels, size=group_4_logs, replace = TRUE))

subs <- bind_rows(group_1, group_2, group_3, group_4)
head(subs)
```

## Part III: K-means  
Now we get to the good part. We've just created a weblog with 50,000 records. We can do the clustering process on this data, but we'll have to start by tranforming it into the right format. 

In particular, kmeans requires that the data be uniformly scaled. So if some subscribers are reading hundreds of articles and others are reading just a few, that's going to distort our results. To weight each subscriber equally, we'll want to render the data as _proportions_ instead of raw weblogs. 

In other words, we're going to look at the percentage of each reader's total reads that fell into each channel, and then compare that. 

### Transform data to show proportion of reading in each channel. 
This part is a little tricky. Essentially we're first counting the views by subscriber, 
then counting the percentage of those views that took place in each channel. 
```{r}
sub_prop <- subs %>%
    select(-sys_time) %>% # we don't need dates for this part
    filter(!is.na(channel)) %>% # toss records where the channel is NA
    group_by(sub_code) %>% 
    mutate(sub_views = sum(n())) %>% # get the total reads per subscriber
    group_by(sub_code, channel, sub_views) %>%
    summarise(sub_channel_views = n()) %>% # get each subscriber's reads in each channel
    mutate(freq = sub_channel_views/sub_views) %>% # render these numbers as a percentage
    arrange(sub_code, desc(freq)) 

head(sub_prop)
```

Now we take each channel's proportions and put that in its own column. We can use `tidyr::spread()` to get the data into this so-called wide format. 
```{r}
# prep data for k-means
subs_wide <- sub_prop %>%
    select(-sub_views, -sub_channel_views) %>%
    spread(channel, freq) 

# Replace NAs with 0
subs_wide[is.na(subs_wide)] <- 0.0

head(subs_wide)
```
### Execute kmeans algorithm
After all that munging, the actual algorithm takes just one line of code to apply. 

Note that we run the algorithm on columns 2 to 7. We're excluding column 1 because kmeans can only accept numeric data types. Including the subscriber codes in the input will cause R to throw an error. 

The plot shows how well our data falls into $k$ clusters. In our case, the data cluster fairly well but notice that there are some ambiguous points. Many of these are the readers in group 4, which read equally in all channels and add a bit of noise to the signal here.  

```{r}
# do kmeans
k_means <- kmeans(subs_wide[2:7], centers = k, nstart = 10, algorithm = "Hartigan-Wong")

# plot kmeans
plotcluster(subs_wide[2:7], k_means$cluster)
```


### Line plot for centers of clusters
Now we are ready to plot the "centroids" returned by kmeans. For each cluster, the kmeans algorithm will return a mean. In our case, this is a mean in n-dimensional space, where $n$ is equal to our number of channels. 

The way to think about this visual output is that each line represents a typical profile for a cluster. The yellow line shows a group that almost exclusively reads about salads. The light blue line shows a second group that mostly reads about seafood, but likes to pair fish with a soup. The dark blue line contains more balanced readers who like sandwiches and soups but read a bit of everything (most of group four will get lumped in with these folks). 
```{r}
# Prepare data for plotting
cent <- k_means$centers %>%
    data.frame() %>%
    rownames_to_column(var = "cluster") %>%
    gather(service, freq, -cluster)

(cluster_count <- length(unique(cent$cluster)))

ggplot(cent, aes(x = service, y = freq, group = cluster)) +
    theme_classic() +
    scale_y_continuous(limits = c(0, 1)) +
    geom_line(aes(color = cluster, size = 1)) +
    guides(size = FALSE) +
    labs(x = "Content Channel") +
    ggtitle(paste0("Centers of the ", cluster_count, " Channel Clusters Generated by KMeans")) +
    scale_color_manual(values = wes_palette("Zissou1"))
```

## Part IV: Map clusters back to original data
Once we have classified our subscribers with the kmeans algorithm, we can ask questions about how those clusters behave. Perhaps the first question to ask is how much variation there is within each cluster. Do these clusters really hold up, when we take them back to the original weblog data?

### Add classifiers back to original dataset  
Appending the cluster numbers is as simple as binding the classification tags back to the original data, since we haven't changed the order of the rows. 
```{r}
# Add cluster numbers back to original dataset
channel_cluster <- cbind(subs_wide, clusterNum = k_means$cluster)

# View the data
head(channel_cluster)
```

### Boxplots of original data clustered  
Now we're ready to look at how the clusters behave. We can ask how many subscribers are in each cluster, for example. Cluster one is significantly larger than the other two. The salad-eating members of group 3 are the least numerous. 

```{r fig.height=4, fig.width=4}
channel_cluster %>%
    group_by(clusterNum) %>%
    summarize(n = n())
```

We can make boxplots that show the variance and outliers in each channel, for each of our clusters. 

A few thoughts on how to read this plot:

- The three plots stacked on top of each other show data for each of the three clusters. 
- We see more outliers and taller boxes for the reading habits of the generalists than for the other two clusters. 
- The salad eaters show very little variation. For the most part they're just reading about how to make salads, and ignoring the rest of the content. 
```{r}
# This step reverses the `spread()` we did to prep the data for kmeans
mapped_data <- channel_cluster %>%
    gather(channel, freq, -clusterNum, -sub_code) 

# Convert the cluster numbers to a factor, and name its levels.
mapped_data$clusterNum <- as.factor(mapped_data$clusterNum)
levels(mapped_data$clusterNum) <- c("Generalists", "Seafood Fans", "Salad Eaters")

# Create a boxplot to view variances and outliers
ggplot(mapped_data, aes(x = channel, y = freq, fill = channel)) +
    facet_wrap(~clusterNum, dir = 'v') +
    geom_boxplot() +
    scale_fill_manual(values = wes_palette(n= 6, name = "IsleofDogs1"))
```

