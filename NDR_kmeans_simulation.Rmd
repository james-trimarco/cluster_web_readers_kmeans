---
title: "Clustering Web Readers With Kmeans"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})

author: "James Trimarco"
date: "2/6/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction  
Back when I was a magazine editor, I spent a lot of time looking over web traffic with my teammates. We could see patterns in the traffic and we often imagined that readers might fall into different categories: people who mostly read lifestyle stories but also read about the environment; people who read politics stories but avoided arts and culture stories, and so on. If we could sort readers into these categories, we might be able to better personalize our communications with them. 

We didn't quite have a way to quantify these hunches, though, so hunches they remained. 

After going back to school for data science, I realized that the kmeans algorithm offers a tool to do exactly the process we'd been thinking about. Kmeans starts by randomly selecting $k$ center points — $k$ is a number that we pick ourselves — and then runs an algorithm that finds all the points closest to each center, takes the average of those points to find a new center, and repeats under stable clusters emerge. (For a full description of how kmeans works, watch this [fun video by StatQuest](https://www.youtube.com/watch?v=4b5d3muPQmA)). 

Since that time, I've run the kmeans algorithm for clients to help them find the clusters hidden in their web traffic data. Usually it's clear that the results are pretty close to the publishers' hunches — "There seemed to be a group that came around every time our star blogger posted." But some of the results usually come as a surprise. 

The following workflow shows the basics of how to apply the kmeans algorithm to web traffic data using synthetic data that roughly — very roughly, actually — simulates some client data I've worked with. 

## Load libraries
```{r libraries, include=FALSE}
library(tidyverse) # for working with data
library(cluster) # clustering algorithms
library(fpc) # for visualizing clusters
library(wesanderson) # for colors
```


## Control panel
Various key settings are set here. 

The four groups of clients represent clusters with different reading habits. In an effort to simulate real web traffic data, the clusters are not all the same size. Furthermore, some clusters read more than others, and that number is independent of the number of readers in the cluster. 
```{r}
# Here's where we set the "k" in kmeans. 
# We'll get a number of clusters equal to the number we set for k here. 
k <- 3

# This sets the number of unique clients to generate
total_subscribers <- 1000
# This sets the number of weblogs the algorithm will examine
total_weblogs <- 50000

# These settings determine how many of the clients fall into each cluster
group_1_cutoff <- (0:250)
group_2_cutoff <- (251:600)
group_3_cutoff <- (601:750)
group_4_cutoff <- c(751:1000)

# These settings determine the number of weblogs made by each 
# group of readers
group_1_logs <- total_weblogs*.4
group_2_logs <- total_weblogs*.2
group_3_logs <- total_weblogs*.3
group_4_logs <- total_weblogs*.1
```

## Synthesize data
### Create publication categories
Please forgive the generic publication categories. Note that some stories are coded NA, which is something I see a lot in real client data. 
```{r}
channels <- c("breakings_news", 
             "arts_culture", 
             "environment", 
             "sports", 
             "blogs", 
             "classifieds", 
             NA)

pr_group_1 <- c(.4, .075, .1, .2, .05, .1, .075)
pr_group_2 <- c(.12, .025, .05, .01, .5, .08, .175)
pr_group_3 <- c(.26, .01, .6, .01, .04, .01, .07)

```


### Create alphanumeric subscriber codes
```{r}
r <- replicate(total_subscribers, paste(sample(LETTERS, 3, replace=TRUE), collapse=""))
n <- replicate(total_subscribers, paste(sample(c(1:9), 3, replace=TRUE), collapse=""))
subscriber_codes <- paste0(r, n)
```

### Create column of dates
```{r}
# This function will generate a uniform sample of dates from 
# within a designated start and end date:
rand.date=function(start.day,end.day,size){   
  days=seq.Date(as.Date(start.day),as.Date(end.day),by="day")  
  pick.day=runif(n=size,min=1,max=length(days)) 
  print(head(pick.day))
  date=days[pick.day]  
}
```

### Put it together into a data frame
```{r}
group_1 <- tibble(sub_code = sample(subscriber_codes[group_1_cutoff], size=group_1_logs, replace = TRUE), 
               sys_time = rand.date("2014-01-01","2018-12-31",group_1_logs), 
               channel = sample(channels, size=group_1_logs, prob = pr_group_1, replace = TRUE))

group_2 <- tibble(sub_code = sample(subscriber_codes[group_2_cutoff], size=group_2_logs, replace = TRUE), 
               sys_time = rand.date("2014-01-01","2018-12-31",group_2_logs), 
               channel = sample(channels, size=group_2_logs, prob = pr_group_2, replace = TRUE))

group_3 <- tibble(sub_code = sample(subscriber_codes[group_3_cutoff], size=group_3_logs, replace = TRUE), 
               sys_time = rand.date("2014-01-01","2018-12-31",group_3_logs), 
               channel = sample(channels, size=group_3_logs, prob = pr_group_3, replace = TRUE))

group_4 <- tibble(sub_code = sample(subscriber_codes[group_4_cutoff], size=group_4_logs, replace = TRUE), 
               sys_time = rand.date("2014-01-01","2018-12-31",group_4_logs), 
               channel = sample(channels, size=group_4_logs, replace = TRUE))

subs <- bind_rows(group_1, group_2, group_3, group_4)
```


# Part III: K-means
### Look for clusters in top 5 most-read titles by customer
```{r}
# 
sub_prop <- subs %>%
    select(-sys_time) %>%
    filter(!is.na(channel)) %>%
    group_by(sub_code) %>%
    mutate(sub_views = sum(n())) %>% 
    group_by(sub_code, channel, sub_views) %>%
    summarise(sub_channel_views = n()) %>%
    mutate(freq = sub_channel_views/sub_views) %>%
    arrange(sub_code, desc(freq)) 

# prep data for k-means
subs_wide <- sub_prop %>%
    select(-sub_views, -sub_channel_views) %>%
    spread(channel, freq) 

# Replace NAs with 0
subs_wide[is.na(subs_wide)] <- 0.0

# do kmeans
k2 <- kmeans(subs_wide[2:7], centers = k, nstart = 10, algorithm = "Hartigan-Wong")

# plot kmeans
plotcluster(subs_wide[2:7], k2$cluster)

# Add cluster numbers back to original dataset
channel_cluster <- cbind(subs_wide, clusterNum = k2$cluster)

# View the data
head(channel_cluster, 50)
```


### Line plot for centers of clusters
```{r}
# Prepare data for plotting
cent <- k2$centers %>%
    data.frame() %>%
    rownames_to_column(var = "cluster") %>%
    gather(service, freq, -cluster)

(cluster_count <- length(unique(cent$cluster)))

ggplot(cent, aes(x = service, y = freq, group = cluster)) +
    geom_line(aes(color = cluster, size = 1)) +
    ggtitle(paste0("Centers of the ", cluster_count, " Service Clusters Generated by KMeans")) +
    scale_color_manual(values = wes_palette("Zissou1"))

```

