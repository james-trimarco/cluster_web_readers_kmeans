---
title: "Clustering Web Readers With Kmeans"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})

author: "James Trimarco"
date: "2/6/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction  
Back when I was a magazine editor, I spent a lot of time looking over web traffic with my teammates. We could see patterns in the traffic and we often imagined that readers might fall into different categories. For example, some people mostly read lifestyle stories but also read about the environment; others who read all the politics stories but avoided arts and culture stories; and so on. If we could sort readers into these categories, we might be able to better personalize our communications with them. 

We didn't quite have a way to quantify these hunches, though, so hunches they remained. 

After going back to school for data science, I realized that the kmeans algorithm offers a tool to do the classification we'd been thinking about. Kmeans starts by randomly selecting $k$ center points — $k$ is a number that we pick ourselves — and then runs an algorithm that finds all the points closest to each center, takes the average of those points to find a new center, and repeats under stable clusters emerge. It's an "unsupervised" algorithm, which means we won't know what categories we're going to find until we actually find them (For a full description of how kmeans works, I recommend this [fun video by StatQuest](https://www.youtube.com/watch?v=4b5d3muPQmA)). 

Since that time, I've run the kmeans algorithm for clients to help them identify the clusters of subscribers hidden in their web traffic data. Usually some results match the publishers' hunches — "We always thought we saw a group that came around every time our star blogger posted new content." But other results came as a surprise. 

The following workflow shows the basics of how to apply the kmeans algorithm to web traffic data using synthetic data that roughly simulates some client data I recently saw. 

## Load libraries
```{r libraries, include=FALSE}
library(tidyverse) # essential data tools
library(cluster) # clustering algorithms
library(fpc) # for visualizing clusters
library(wesanderson) # for colors
```


## Control panel
Various key settings are set here. 

The four groups of clients we create in the code chunk below represent clusters with different reading habits. In an effort to simulate real web traffic data, the clusters are not all the same size. Furthermore, some clusters read more than others, and that number is independent of the number of readers in the cluster. 
```{r}
# Here's where we set the "k" in kmeans. 
# At the end, we'll see a number of clusters equal to the number we set k to here. 
k <- 3

# This sets the number of unique readers to generate.
# Eacher reader will get a subscriber code.
total_subscribers <- 1000

# This sets the number of weblogs the algorithm will examine.
# On average, each subscriber read 50 pieces of content during this time period.
total_weblogs <- 50000

# These settings determine how many of the clients fall into each cluster.
group_1_cutoff <- (0:250)
group_2_cutoff <- (251:600)
group_3_cutoff <- (601:750)
group_4_cutoff <- c(751:1000)

# These settings determine the number of weblogs made by each 
# group of readers. 
# The coefficients here should sum to 1.
group_1_logs <- .4*total_weblogs
group_2_logs <- .3*total_weblogs
group_3_logs <- .2*total_weblogs
group_4_logs <- .1*total_weblogs
```

## Synthesize weblog data
### Create publication categories
Imagine a blog that publishes recipes, and has different channels for different types of food. The probability vectors below define the proportions with which members of each group read the articles in different channels. 

Group Four doesn't have a probability matrix; its readers select from all the categories equally. 

Note that some stories are coded NA, which is something you see a lot in real client data. 

```{r}
channels <- c("soups", 
             "sandwiches", 
             "salads", 
             "barbecue", 
             "seafood", 
             "stir_fry", 
             NA)

# Create probability vectors
# Each of these sums to one
pr_group_1 <- c(.4, .3, .1, .05, .05, .025, .075)
pr_group_2 <- c(.12, .025, .05, .01, .5, .12, .175)
pr_group_3 <- c(.01, .01, .6, .01, .04, .01, .07)
```


### Create alphanumeric subscriber codes
The subscriber codes have six positions, with three letters and three numbers. 
```{r}
alpha_bit <- replicate(total_subscribers, paste(sample(LETTERS, 3, replace=TRUE), collapse=""))
numeric_bit <- replicate(total_subscribers, paste(sample(c(1:9), 3, replace=TRUE), collapse=""))
subscriber_codes <- paste0(alpha_bit, numeric_bit)
```

### Create column of dates
```{r}
# This function will generate a uniform sample of dates from 
# within a designated start and end date:
rand.date=function(start.day,end.day,size){   
  days=seq.Date(as.Date(start.day),as.Date(end.day),by="day")  
  pick.day=runif(n=size,min=1,max=length(days)) 
  date=days[pick.day]  
}
```

### Put it together into a data frame
This step glues the vectors we've been building into a data frame where each observation is a website visit. Each row has a subscriber code, a date, and a channel showing the kind of recipe the subscriber was reading.  
```{r}
group_1 <- tibble(sub_code = sample(subscriber_codes[group_1_cutoff], size=group_1_logs, replace = TRUE), 
               sys_time = rand.date("2014-01-01","2018-12-31",group_1_logs), 
               channel = sample(channels, size=group_1_logs, prob = pr_group_1, replace = TRUE))

group_2 <- tibble(sub_code = sample(subscriber_codes[group_2_cutoff], size=group_2_logs, replace = TRUE), 
               sys_time = rand.date("2014-01-01","2018-12-31",group_2_logs), 
               channel = sample(channels, size=group_2_logs, prob = pr_group_2, replace = TRUE))

group_3 <- tibble(sub_code = sample(subscriber_codes[group_3_cutoff], size=group_3_logs, replace = TRUE), 
               sys_time = rand.date("2014-01-01","2018-12-31",group_3_logs), 
               channel = sample(channels, size=group_3_logs, prob = pr_group_3, replace = TRUE))

group_4 <- tibble(sub_code = sample(subscriber_codes[group_4_cutoff], size=group_4_logs, replace = TRUE), 
               sys_time = rand.date("2014-01-01","2018-12-31",group_4_logs), 
               channel = sample(channels, size=group_4_logs, replace = TRUE))

subs <- bind_rows(group_1, group_2, group_3, group_4)
head(subs)
```

# Part III: K-means
### Transform data to show proportion of reading in each channel. 

```{r}
# 
sub_prop <- subs %>%
    select(-sys_time) %>%
    filter(!is.na(channel)) %>%
    group_by(sub_code) %>%
    mutate(sub_views = sum(n())) %>% 
    group_by(sub_code, channel, sub_views) %>%
    summarise(sub_channel_views = n()) %>%
    mutate(freq = sub_channel_views/sub_views) %>%
    arrange(sub_code, desc(freq)) 

# prep data for k-means
subs_wide <- sub_prop %>%
    select(-sub_views, -sub_channel_views) %>%
    spread(channel, freq) 

# Replace NAs with 0
subs_wide[is.na(subs_wide)] <- 0.0
```

```{r}
# do kmeans
k2 <- kmeans(subs_wide[2:7], centers = k, nstart = 10, algorithm = "Hartigan-Wong")

# plot kmeans
plotcluster(subs_wide[2:7], k2$cluster)

# Add cluster numbers back to original dataset
channel_cluster <- cbind(subs_wide, clusterNum = k2$cluster)

# View the data
head(channel_cluster, 50)
```


### Line plot for centers of clusters
```{r}
# Prepare data for plotting
cent <- k2$centers %>%
    data.frame() %>%
    rownames_to_column(var = "cluster") %>%
    gather(service, freq, -cluster)

(cluster_count <- length(unique(cent$cluster)))

ggplot(cent, aes(x = service, y = freq, group = cluster)) +
    geom_line(aes(color = cluster, size = 1)) +
    ggtitle(paste0("Centers of the ", cluster_count, " Service Clusters Generated by KMeans")) +
    scale_color_manual(values = wes_palette("Zissou1"))

```

